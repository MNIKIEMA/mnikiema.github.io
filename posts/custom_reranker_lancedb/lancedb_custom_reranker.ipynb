{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a07c45",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title:  Batching with SBERT and LanceDB\n",
    "description: Retrieval with SBERT and LanceDB\n",
    "author: \"Mahamadi NIKIEMA\"\n",
    "thumbnail-img: profile.jpg\n",
    "tags: [Reranking, Python, Embeddings]\n",
    "date:   2025-05-14 21:55:51 +0200\n",
    "categories: scraping\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42531685",
   "metadata": {},
   "source": [
    "I start using lancedb for many projects because of the easy setup and simplicity. I used it with open-source models available on HuggingFace and noticed that it is sometimes slow while running some retrieval tasks.\n",
    "\n",
    "[Sentence Transformers v4.1](https://github.com/UKPLab/sentence-transformers/releases/tag/v4.1.0) release bring multiple backend support of the SBERT model.\n",
    "Now we can use some backend such as O``NNX`` and ``OpenVINO`` to speed up the inference. As shown in the [benchmark](https://sbert.net/docs/cross_encoder/usage/efficiency.html) the speed-up gains is ``*1.73x*`` on the GPU wile preserving 99.61% of the accuracy. We can now use the SBERT model with the LanceDB backend to speed up the retrieval process.\n",
    "\n",
    "The native integration of the SBERT model with LanceDB is available but the backend support is not available and they provide a [documentation](https://lancedb.github.io/lancedb/reranking/custom_reranker/#example-of-a-custom-reranker) to write a custom reranker.\n",
    "\n",
    "I will show you how to write a custom reranker using SBERT and LanceDB for ``ONNX`` backend.\n",
    "\n",
    "As the ``cross-encoder`` model is already implemented, I took inspiration from the [cross-encoder](https://lancedb.github.io/lancedb/reranking/cross_encoder/) example to implement the ``ONNX`` backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dbe8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Optional\n",
    "import pyarrow as pa\n",
    "import lancedb\n",
    "from lancedb.rerankers import Reranker\n",
    "from functools import cached_property\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "class ONNXCrossEncoderReranker(Reranker):\n",
    "    \"\"\"\n",
    "    A custom reranker for LanceDB that uses an ONNX backend for cross-encoder models.\n",
    "\n",
    "    This reranker provides:\n",
    "    1. Increased performance through ONNX runtime\n",
    "    2. Flexibility to filter results based on criteria\n",
    "    3. Support for various cross-encoder models\n",
    "    4. Batch processing for efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        max_length: int = 256,\n",
    "        batch_size: int = 32,\n",
    "        device: Optional[str] = None,\n",
    "        model_kwargs: Optional[dict] = None,\n",
    "        trust_remote_code: bool = True,\n",
    "        column: str = \"text\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ONNX Cross-Encoder Reranker.\n",
    "\n",
    "        Args:\n",
    "            model_name_or_path: Original model name or path for tokenization\n",
    "            onnx_model_path: Path to the ONNX model file\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "            filters: String or list of strings to filter out from results\n",
    "            batch_size: Number of examples to process at once\n",
    "            score_threshold: Minimum score threshold for results\n",
    "            device: Device to run inference on ('cpu', 'cuda', etc.)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model_kwargs = model_kwargs if model_kwargs is not None else {}\n",
    "        self.max_length = max_length\n",
    "        self.column = column\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.trust_remote_code = trust_remote_code\n",
    "        if self.device is None:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        # Allows overriding the automatically selected device\n",
    "        cross_encoder = CrossEncoder(\n",
    "            model_name_or_path=self.model_name,\n",
    "            backend=\"onnx\",\n",
    "            device=self.device,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "        )\n",
    "\n",
    "        return cross_encoder\n",
    "\n",
    "    def _rerank(self, result_set: pa.Table, query: str):\n",
    "        result_set = self._handle_empty_results(result_set)\n",
    "        if len(result_set) == 0:\n",
    "            return result_set\n",
    "        passages = result_set[self.column].to_pylist()\n",
    "        cross_inp = [[query, passage] for passage in passages]\n",
    "        cross_scores = self.model.predict(cross_inp)\n",
    "        result_set = result_set.append_column(\n",
    "            \"_relevance_score\", pa.array(cross_scores, type=pa.float32())\n",
    "        )\n",
    "\n",
    "        return result_set\n",
    "\n",
    "    def rerank_hybrid(\n",
    "        self,\n",
    "        query: str,\n",
    "        vector_results: pa.Table,\n",
    "        fts_results: pa.Table,\n",
    "    ):\n",
    "        combined_results = self.merge_results(vector_results, fts_results)\n",
    "        combined_results = self._rerank(combined_results, query)\n",
    "        # sort the results by _score\n",
    "        if self.score == \"relevance\":\n",
    "            combined_results = self._keep_relevance_score(combined_results)\n",
    "        elif self.score == \"all\":\n",
    "            raise NotImplementedError(\"return_score='all' not implemented for CrossEncoderReranker\")\n",
    "        combined_results = combined_results.sort_by([(\"_relevance_score\", \"descending\")])\n",
    "\n",
    "        return combined_results\n",
    "\n",
    "    def rerank_vector(self, query: str, vector_results: pa.Table):\n",
    "        vector_results = self._rerank(vector_results, query)\n",
    "        if self.score == \"relevance\":\n",
    "            vector_results = vector_results.drop_columns([\"_distance\"])\n",
    "\n",
    "        vector_results = vector_results.sort_by([(\"_relevance_score\", \"descending\")])\n",
    "        return vector_results\n",
    "\n",
    "    def rerank_fts(self, query: str, fts_results: pa.Table):\n",
    "        fts_results = self._rerank(fts_results, query)\n",
    "        if self.score == \"relevance\":\n",
    "            fts_results = fts_results.drop_columns([\"_score\"])\n",
    "\n",
    "        fts_results = fts_results.sort_by([(\"_relevance_score\", \"descending\")])\n",
    "        return fts_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad80f8",
   "metadata": {},
   "source": [
    "Let us try it out with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d27fcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(\"./lancedb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d7d37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "\n",
    "func = get_registry().get(\"sentence-transformers\").create(name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                                          device=\"cpu\")\n",
    "\n",
    "# Define a Schema\n",
    "class Words(LanceModel):\n",
    "    # This is the source field to compute the embeddings and index\n",
    "    text: str = func.SourceField()\n",
    "\n",
    "    # This is the vector field that will store the output of the embeddings\n",
    "    vector: Vector(func.ndims()) = func.VectorField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6aa9bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"text\": \"This guy is happy\"},\n",
    "    {\"text\": \"This person is not happy\"},\n",
    "    {\"text\": \"That is a very happy person\"},\n",
    "    {\"text\": \"This is a good guy\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0261c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = db.create_table(\"testing\", schema=Words, mode=\"overwrite\")\n",
    "table.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "277d0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = ONNXCrossEncoderReranker(model_name=\"Alibaba-NLP/gte-reranker-modernbert-base\",\n",
    "                                         model_kwargs={\"file_name\": \"onnx/model_int8.onnx\"})\n",
    "question = \"This is a happy person\"\n",
    "results = table.search(question, query_type=\"vector\").limit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "315c419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          text  _distance\n",
      "0  That is a very happy person   0.394280\n",
      "1            This guy is happy   0.515812\n",
      "2     This person is not happy   0.548586\n",
      "3           This is a good guy   0.982840\n"
     ]
    }
   ],
   "source": [
    "print(results.to_pandas().drop(\"vector\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0c864a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many ONNX model files were found in onnx/model.onnx ,onnx/model_bnb4.onnx ,onnx/model_fp16.onnx ,onnx/model_int8.onnx ,onnx/model_q4.onnx ,onnx/model_q4f16.onnx ,onnx/model_quantized.onnx ,onnx/model_uint8.onnx. specify which one to load by using the `file_name` and/or the `subfolder` arguments. Loading the file model_int8.onnx in the subfolder onnx.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66158b1a522440b5b7487d035dc31218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_int8.onnx:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          text  _relevance_score\n",
      "0  That is a very happy person          0.938593\n",
      "1            This guy is happy          0.916949\n",
      "2           This is a good guy          0.885104\n",
      "3     This person is not happy          0.819631\n"
     ]
    }
   ],
   "source": [
    "print(results.rerank(reranker=cross_encoder).to_pandas().drop(\"vector\", axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnikiema-github-io",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
