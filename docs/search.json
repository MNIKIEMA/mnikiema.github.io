[
  {
    "objectID": "draft/registry.html",
    "href": "draft/registry.html",
    "title": "Decorator-based method",
    "section": "",
    "text": "How to load different model of architecture? - We can use if/else - Or using importlib.import_module with getattr to access to the class\nimport torch.nn as nn\n\nclass ResNet18(nn.Module):\n    def __init__(self):\n        super().__init__()\n        print(\"Resnet18\")\n\n\nclass ResNet32(nn.Module):\n    def __init__(self):\n        super().__init__()\n        print(\"Resnet32\")\n\n\nclass ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        print(\"Resnet50\")"
  },
  {
    "objectID": "draft/registry.html#model-registry",
    "href": "draft/registry.html#model-registry",
    "title": "Decorator-based method",
    "section": "Model Registry",
    "text": "Model Registry\nRegistry simplifies the loading with runtime or config.\n\n# Model Registry Implementation\nclass ModelRegistry:\n    _registry = {}\n\n    @classmethod\n    def register(cls, name, model_class):\n        cls._registry[name] = model_class\n\n    @classmethod\n    def get_model(cls, name):\n        model_class = cls._registry.get(name)\n        if model_class:\n            return model_class()\n        else:\n            raise ValueError(f\"Model type '{name}' not registered.\")\n\nThen we can register models as follows.\n\nModelRegistry.register('resnet18', ResNet18)\nModelRegistry.register('resnet32', ResNet32)\nModelRegistry.register('resnet50', ResNet50)\n\n\nmodel = ModelRegistry.get_model(name=\"resnet18\")\n\nResnet18"
  },
  {
    "objectID": "draft/registry.html#ref",
    "href": "draft/registry.html#ref",
    "title": "Decorator-based method",
    "section": "Ref",
    "text": "Ref\n\nhttps://www.abhik.ai/articles/registry-pattern\nhttps://charlesreid1.github.io/python-patterns-the-registry.html"
  },
  {
    "objectID": "draft/data_collators copy.html",
    "href": "draft/data_collators copy.html",
    "title": "Outline",
    "section": "",
    "text": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import DefaultDataCollator, AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerBase\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ntokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntext: list[str] = [\"A cute dog\", \"My cat is cute\"]\nchat = [[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"A cute dog\"}],\n        [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"My cat is cute\"}]\n    ]\ntokenized = tokenizer.apply_chat_template(conversation=chat, return_dict=True)\ninput_ids = tokenized[\"input_ids\"]\nfor tokens in input_ids:\n    print(len(tokens))\nThe default data collator in Hugging Face is initialized as follows\ncollate_fn = DefaultDataCollator()\nWe can tokenize the text using the tokenizer and pass it to the data collator\ntokens = [tokenizer(t) for t in text]\nprint(tokens)\n\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    print(batch)\n    break\n\n[{'input_ids': [49, 25489, 2767], 'attention_mask': [1, 1, 1]}, {'input_ids': [5965, 2644, 314, 25489], 'attention_mask': [1, 1, 1, 1]}]\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 5\n      2 print(tokens)\n      4 dataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\n----&gt; 5 for batch in dataloader:\n      6     print(batch)\n      7     break\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733, in _BaseDataLoaderIter.__next__(self)\n    730 if self._sampler_iter is None:\n    731     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    732     self._reset()  # type: ignore[call-arg]\n--&gt; 733 data = self._next_data()\n    734 self._num_yielded += 1\n    735 if (\n    736     self._dataset_kind == _DatasetKind.Iterable\n    737     and self._IterableDataset_len_called is not None\n    738     and self._num_yielded &gt; self._IterableDataset_len_called\n    739 ):\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789, in _SingleProcessDataLoaderIter._next_data(self)\n    787 def _next_data(self):\n    788     index = self._next_index()  # may raise StopIteration\n--&gt; 789     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    790     if self._pin_memory:\n    791         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n---&gt; 55 return self.collate_fn(data)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:125, in DefaultDataCollator.__call__(self, features, return_tensors)\n    123 if return_tensors is None:\n    124     return_tensors = self.return_tensors\n--&gt; 125 return default_data_collator(features, return_tensors)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:93, in default_data_collator(features, return_tensors)\n     87 # In this function we'll make the assumption that all `features` in the batch\n     88 # have the same attributes.\n     89 # So we will look at the first element as a proxy for what attributes exist\n     90 # on the whole batch.\n     92 if return_tensors == \"pt\":\n---&gt; 93     return torch_default_data_collator(features)\n     94 elif return_tensors == \"tf\":\n     95     return tf_default_data_collator(features)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:159, in torch_default_data_collator(features)\n    157             batch[k] = torch.from_numpy(np.stack([f[k] for f in features]))\n    158         else:\n--&gt; 159             batch[k] = torch.tensor([f[k] for f in features])\n    161 return batch\n\nValueError: expected sequence of length 3 at dim 1 (got 4)\nWe are getting errors because the data samples have different lengths. The default data collator in Hugging Face is very simple. - It does not handle variable length data. - It does not handle padding.\nTo solve this problem, we can use a padding technique to get the same length for all samples in a batch. Hugging Face provides a built-in data collator called DataCollatorWithPadding that can handle this padding automatically.\nThe DataCollatorWithPadding handle tokenization and padding automatically. - We can handle padding stategy, we can set pad_to_multiple_of, the return type and the maximum length allowed.\nfrom transformers import DataCollatorWithPadding\ncollate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    print(input_ids)\n    print(batch.keys())\n    decoded = tokenizer.batch_decode(input_ids)\n    print(decoded)\n    break\n\ntensor([[   49, 25489,  2767,     2,     2,     2,     2,     2],\n        [ 5965,  2644,   314, 25489,     2,     2,     2,     2]])\ndict_keys(['input_ids', 'attention_mask'])\n['A cute dog&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;', 'My cat is cute&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;']\nIf you are fine-tuning a decoder model for language modeling, the collator used is DataCollatorForLanguageModeling. Let’s see how it works by importing the DataCollatorForLanguageModeling\nfrom transformers import DataCollatorForLanguageModeling\nWe can use it by setting mlm to False. mlm=False means that we are fine-tuning a decoder model for language modeling and not a masked language model such as BERT.\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    all_tokens = tokenizer.batch_decode(input_ids)\n    print(batch)\n    for token in all_tokens:\n        print(token)\n    break\n\n{'input_ids': tensor([[   49, 25489,  2767,     2],\n        [ 5965,  2644,   314, 25489]]), 'attention_mask': tensor([[1, 1, 1, 0],\n        [1, 1, 1, 1]]), 'labels': tensor([[   49, 25489,  2767,  -100],\n        [ 5965,  2644,   314, 25489]])}\nA cute dog&lt;|im_end|&gt;\nMy cat is cute\ntokenizer.padding_side = \"left\"\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    all_tokens = tokenizer.batch_decode(input_ids)\n    print(batch)\n    for token in all_tokens:\n        print(token)\n    break\n\n{'input_ids': tensor([[    2,    49, 25489,  2767],\n        [ 5965,  2644,   314, 25489]]), 'attention_mask': tensor([[0, 1, 1, 1],\n        [1, 1, 1, 1]]), 'labels': tensor([[ -100,    49, 25489,  2767],\n        [ 5965,  2644,   314, 25489]])}\n&lt;|im_end|&gt;A cute dog\nMy cat is cute\nfrom typing import cast, Dict, Any\nfrom trl.trainer.sft_config import SFTConfig\nfrom trl.trainer.sft_trainer import SFTTrainer\nfrom datasets import Dataset\n\nraw_data: list[Dict[str, str]] = [\n    {\"source\": \"Moi\", \"target\": \"Me\"},\n    {\"source\": \"What is your name?\", \"target\": \"Comment tu t'appelles?\"},\n    {\"source\": \"I love programming.\", \"target\": \"J'adore la programmation.\"},\n]\n\ndata = Dataset.from_list(raw_data)\ndef count_tokens(example):\n    src_ids = tokenizer(\n        example[\"source\"],\n        add_special_tokens=False,\n        padding=False,\n        truncation=False,\n    )[\"input_ids\"]\n\n    tgt_ids = tokenizer(\n        example[\"target\"],\n        add_special_tokens=False,\n        padding=False,\n        truncation=False,\n    )[\"input_ids\"]\n\n    return {\n        \"source_tokens\": len(src_ids),\n        \"target_tokens\": len(tgt_ids),\n        \"sum_tokens\": len(src_ids) + len(tgt_ids),\n    }\n\ndata = data.map(count_tokens)\n\nprint(data)\n\n\n\n\nDataset({\n    features: ['source', 'target', 'source_tokens', 'target_tokens', 'sum_tokens'],\n    num_rows: 3\n})\nINSTRUCTION = \"Translate the following English text to French.\"\n\ndef formatting_prompts_func(examples, tokenizer: PreTrainedTokenizerBase,\n                            add_generation_prompt=False, tokenize=False,\n                            return_dict=False, tokenizer_kwargs: Dict[str, Any] | None = None,\n                            return_tensors: str | None=None):\n    src_texts = examples['source']\n    trg_texts = examples['target']\n\n    chat = [\n        [\n        {\"role\": \"system\", \"content\": \"Translate into French.\",},\n        {\"role\": \"user\", \"content\": f\"{src}\"},\n        {\"role\": \"assistant\", \"content\": trg},\n        ]\n        for src, trg in zip(src_texts, trg_texts, strict=True)\n    ]\n\n    texts = tokenizer.apply_chat_template(\n        chat,\n        tokenize=tokenize,\n        add_generation_prompt=add_generation_prompt,\n        tokenizer_kwargs=tokenizer_kwargs,\n        truncation=True,\n        return_dict=return_dict,\n        return_tensors=return_tensors,\n    )\n    if not tokenize:\n        return {\"text\": [x.removeprefix(tokenizer.bos_token) for x in texts]}\n    return texts\nformatted_data = data.map(\n    lambda examples: formatting_prompts_func(examples, tokenizer),\n    batched=True,   \n    remove_columns=data.column_names,\n)\nprint(formatted_data[0][\"text\"])\n\nsystem\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nMoi&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nMe&lt;|im_end|&gt;\nfrom functools import partial\nformatting_prompts_func_tokenize = partial(\n    formatting_prompts_func, tokenizer=tokenizer,\n    add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=None,\n    tokenizer_kwargs={\"padding_side\": \"left\"}\n)\ntokenized_data = data.map(\n    formatting_prompts_func_tokenize,\n    batched=True,\n    remove_columns=data.column_names,\n)\nfor d in tokenized_data:\n    print(len(d[\"input_ids\"]))\n\n28\n37\n38\nfor d in tokenized_data:\n    ids = d[\"input_ids\"]\n    print(type(ids))\n    print(len(d[\"input_ids\"]))\n\n&lt;class 'list'&gt;\n28\n&lt;class 'list'&gt;\n37\n&lt;class 'list'&gt;\n38\n# tokenized_data.set_format(type=tokenized_data.format[\"type\"], columns=list(tokenized_data.features.keys()))\n# tokenized_data\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\ndataloader = DataLoader(tokenized_data, batch_size=2, collate_fn=collator)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    print(input_ids.shape)\n    labels = batch[\"labels\"]\n    all_tokens = tokenizer.batch_decode(input_ids)\n    for i, token in enumerate(all_tokens):\n        print(token)\n        \n        print(\"---\")\n\ntorch.Size([2, 40])\n&lt;|im_start|&gt;system\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nMoi&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nMe&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;\n---\n&lt;|im_start|&gt;system\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat is your name?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nComment tu t'appelles?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;\n---\ntorch.Size([1, 40])\n&lt;|im_start|&gt;system\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nI love programming.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nJ'adore la programmation.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;|im_end|&gt;&lt;|im_end|&gt;\n---\nprint(tokenizer.padding_side)\n\nright\ntokenizer.padding_side = \"left\"\ncollator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\ndataloader = DataLoader(tokenized_data, batch_size=2, collate_fn=collator)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    print(input_ids.shape)\n    labels = batch[\"labels\"]\n    print(labels.shape)\n    all_tokens = tokenizer.batch_decode(input_ids)\n    for i, token in enumerate(all_tokens):\n        print(token)\n        \n        print(\"---\")\n    break\n\ntorch.Size([2, 40])\ntorch.Size([2, 40])\n&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_start|&gt;system\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nMoi&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nMe&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\n---\n&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_start|&gt;system\nTranslate into French.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nWhat is your name?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nComment tu t'appelles?&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\n---\nprint(tokenizer.padding_side)\nencode = tokenizer([\"Translate into French\", \"Comment tu t'appelles?\"], padding_side=\"right\", padding=True, truncation=True, max_length=10)\nprint(encode)\nprint(tokenizer.padding_side)\n\nleft\n{'input_ids': [[8241, 12837, 618, 3432, 2, 2, 2], [30770, 7269, 252, 23, 1207, 23280, 47]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1]]}\nleft\nbatch = next(iter(dataloader))\n\ninput_ids = batch[\"input_ids\"]\nlabels = batch[\"labels\"]\n\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids)\n    logits = outputs.logits\nprint(logits.shape)\n\ntorch.Size([2, 6, 49152])\nshift_logits = logits[:, :-1, :]\nshift_labels = labels[:, 1:]\nprint(shift_logits.shape)\nprint(shift_labels.shape)\n\ntorch.Size([2, 7, 49152])\ntorch.Size([2, 7])\nfrom torch.nn import CrossEntropyLoss\nCrossEntropy = CrossEntropyLoss()\nloss = CrossEntropy(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\nprint(loss)\n\ntensor(12.9029)"
  },
  {
    "objectID": "draft/data_collators copy.html#warm-up",
    "href": "draft/data_collators copy.html#warm-up",
    "title": "Outline",
    "section": "Warm Up",
    "text": "Warm Up\n\nHF does not stride by default in their data collators.\nWe can stride manually with the tokenizer using the stride and max_length parameters and return_overflowing_tokens=True.\nIf we want to see example like Sebastian did in his blog post, we need a custom data collator that implements striding.\n\nNow we know how to use the data collator for instruction fine-tuning. My use case is to fine-tune a decoder model for translation and we want to evaluate it using some metrics like BLEU. For this, we firstly rely on pandas loop to handle the batching and we want to use the data collator to tokenize the text. How to pass the text and reference to the data collator without loosing some variable like the reference? For this I subclass the data collator and add a new method __call__.\n\nclass TranslationPromptCollator(DataCollatorWithPadding):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        pad_to_multiple_of: int | None = None,\n        return_tensors: str = \"pt\",\n        max_length: int | None = None,\n    ):\n        super().__init__(\n            tokenizer=tokenizer,\n            padding=True,\n            pad_to_multiple_of=pad_to_multiple_of,\n            return_tensors=return_tensors,\n            max_length=max_length,\n        )\n        self.tokenizer = tokenizer\n\n    def __call__(self, features):\n        src_texts = [f[\"src_text\"] for f in features]\n        tgt_texts = [f[\"tgt_text\"] for f in features]\n\n        examples = {\n            \"source\": src_texts,\n            \"target\": tgt_texts,\n        }\n\n        inputs = formatting_prompts_func(\n            examples=examples,\n            tokenizer=self.tokenizer,\n            add_generation_prompt=True,\n            tokenize=True,\n        )\n\n        inputs.pop(\"token_type_ids\", None)\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"],\n            \"references\": tgt_texts,\n            \"sources\": src_texts,\n        }\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 class TranslationPromptCollator(DataCollatorWithPadding):\n      2     def __init__(\n      3         self,\n      4         tokenizer: PreTrainedTokenizerBase,\n   (...)      7         max_length: int | None = None,\n      8     ):\n      9         super().__init__(\n     10             tokenizer=tokenizer,\n     11             padding=True,\n   (...)     14             max_length=max_length,\n     15         )\n\nNameError: name 'DataCollatorWithPadding' is not defined"
  },
  {
    "objectID": "things_i_learned/padding_with_unsloth/index.html",
    "href": "things_i_learned/padding_with_unsloth/index.html",
    "title": "Mahamadi NIKIEMA",
    "section": "",
    "text": "Setup: some text\nIf let trl handle the tokenization, then the padding will be right even if we set tokenizer.padding_side = \"left\"\nIf we want to pad left, then we need to pass the collator with tokenizer.padding_side = \"left\" set before the dataloader\nAnother solution is to pass SFTConfig(..., dataset_kwargs={\"padding_side\": \"left\",})\nUnsloth needs this dataset_num_proc=num_proc to control the number of processes used for tokenization"
  },
  {
    "objectID": "posts/gdiy/2023-08-06-scrap-gdiy.html",
    "href": "posts/gdiy/2023-08-06-scrap-gdiy.html",
    "title": "Scraping the most listened podcast in France, GDIY",
    "section": "",
    "text": "The podcast Generation Do It Yourself is one of the most listen in France. I listen this podcast every weekend and I decide to use my programming skill to scrap audio data from the web site. This post will also guide you to download audio from your favorite podcasts."
  },
  {
    "objectID": "posts/gdiy/2023-08-06-scrap-gdiy.html#prerequisites",
    "href": "posts/gdiy/2023-08-06-scrap-gdiy.html#prerequisites",
    "title": "Scraping the most listened podcast in France, GDIY",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we dive into the details, you need to install the prerequisites by running: pip install requests beautifulsoup4 tqdm"
  },
  {
    "objectID": "posts/gdiy/2023-08-06-scrap-gdiy.html#understanding-the-python-script",
    "href": "posts/gdiy/2023-08-06-scrap-gdiy.html#understanding-the-python-script",
    "title": "Scraping the most listened podcast in France, GDIY",
    "section": "Understanding the Python Script",
    "text": "Understanding the Python Script\nLet’s delve into the Python script that automates the process of downloading podcast episodes. I will introduce you to a class named AudioLoader that is equipped with methods to perform various tasks such as loading, updating, downloading, and processing audio data.\n\nClass Initialization\nThe AudioLoader class initializes with a data_path parameter which is the path to save the downloaded audio files and keep track of loaded episodes. The loaded_episodes set stores the loaded episode names.\nclass AudioLoader(object):\n    def __init__(self, data_path):\n        self.data_path = data_path\n        self.loaded_episodes = self.load_loaded_episodes()\n\n\nLoading and Updating Episode Details\nThe load_loaded_episodes method checks for the existence of a loaded_episodes.json file and creates one if it doesn’t exist, to store the details of loaded episodes.\nThe update_loaded_episodes method updates the loaded_episodes set and JSON file with new episode details.\ndef load_loaded_episodes(self):\n        if not os.path.exists(os.path.join(self.data_path,'loaded_episodes.json')):\n            with open(os.path.join(self.data_path,'loaded_episodes.json'), 'w') as f:\n                json.dump([], f)\n        with open(os.path.join(self.data_path,'loaded_episodes.json'), 'r') as f:\n            return set(json.load(f))\n        \ndef update_loaded_episodes(self, episode_id):\n    self.loaded_episodes.add(episode_id)\n    with open(os.path.join(self.data_path,'loaded_episodes.json'), 'w') as f:\n        json.dump(list(self.loaded_episodes), f)\n\n\nFetching and Processing Data\nThe load_data method fetches all episodes from the podcast’s RSS feed using the requests and BeautifulSoup libraries.\nThe process_data method iterates over all episodes and filters out those with certain phrases in the title (like “[EXTRAIT]”). It also prevents downloading episodes that have already been loaded.\ndef load_data(self, feed_url):\n        page = requests.get(feed_url)\n        soup = BeautifulSoup(page.content, \"xml\")\n        return soup.find_all('item')\n    \ndef process_data(self):\n    all_name = set()\n    audio_info = {}\n    audio_to_skip = [\"[EXTRAIT]\",\"[EXTRACT]\",\"[REDIFF]\"]\n    data = self.load_data(\"https://rss.art19.com/generation-do-it-yourself\")\n    for episode in data:\n        link = episode.find(\"enclosure\")[\"url\"]\n        title = episode.find(\"title\").text\n        episode_id = \" \".join(title.split(\" - \")[:-1]).replace(\"#\", \"\")\n        episode_id = re.sub(r'[%/!@#\\*\\$\\?\\+\\^\\\\\\\\\\\\]', '', episode_id)\n        \n        skip = [skip_audio for skip_audio in audio_to_skip if skip_audio in title]\n        if not skip:\n\n            if episode_id not in self.loaded_episodes:\n                try:\n                    episode_id = self.simplify_name(episode_id)\n                except:\n                    print(title)\n\n                if episode_id in all_name:\n                    episode_id = episode_id+\"-1\"\n                audio_info[episode_id] = title\n                all_name.add(episode_id)\n                self.download_episode(link, episode_id)\n                self.update_loaded_episodes(episode_id)\n    \n    return audio_info\n\n\nDownloading Episodes\nThe download_episode method downloads an episode’s audio file and saves it with a simplified name derived from the title.\ndef download_episode(self, episode_url, audio_name):\n    audio = requests.get(episode_url)\n    with open(os.path.join(self.data_path, audio_name+\".mp3\"), \"wb\") as fp:\n        fp.write(audio.content)\n\n\nSimplifying File Names\nThe simplify_name method simplifies episode names based on certain conditions to create a clean and concise file name for each downloaded audio file.\ndef simplify_name(self, file_name:str):\n    file_name = file_name.strip()\n    if file_name.startswith(\"COVID\"):\n        new_filename = \"-\".join(file_name.split()[:2])\n    elif file_name.lower().startswith(\"hors\"):\n        new_filename = file_name.split()\n        new_filename = \"-\".join(new_filename[:2])\n    elif file_name.startswith(\"Early\"):\n        new_filename = \"-\".join(file_name.split()[:3])\n    else:\n        new_filename = file_name.split()[0]\n    return new_filename"
  },
  {
    "objectID": "posts/gdiy/2023-08-06-scrap-gdiy.html#conclusion",
    "href": "posts/gdiy/2023-08-06-scrap-gdiy.html#conclusion",
    "title": "Scraping the most listened podcast in France, GDIY",
    "section": "Conclusion",
    "text": "Conclusion\nWith this Python script, you can automatically load and download episodes of the Generation Do It Yourself podcast, saving you time and effort. You can customize the script to work with other podcasts by modifying the RSS feed URL and adjusting the naming conventions in the simplify_name method.\nFeel free to extend this script with more features, such as adding metadata to the audio files."
  },
  {
    "objectID": "things_i_learned/index.html",
    "href": "things_i_learned/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Classes Are Instances of type\n\n\n\npython\n\nTIL\n\n\n\nUnderstanding Python’s type and Class Creation\n\n\n\n\n\nFeb 21, 2026\n\n\nMahamadi NIKIEMA\n\n\n\n\n\n\n\n\n\n\n\n\nSerialisation with Msgspec\n\n\n\npython\n\nTIL\n\n\n\nAnother Serializer in Python\n\n\n\n\n\nDec 22, 2024\n\n\nMahamadi NIKIEMA\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post of my blog.\nI’ll use this space to share what I’m learning, exploring, and building."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Mahamadi NIKIEMA",
    "section": "",
    "text": "Normalization in Transformers\n\n\n\nDeep Learning\n\n\n\nNormalization Methods in Transformers\n\n\n\n\n\nAug 15, 2025\n\n\nMahamadi NIKIEMA\n\n\n\n\n\n\n\n\n\n\n\n\nScraping the most listened podcast in France, GDIY\n\n\n\nscraping\n\n\n\nData scraping project\n\n\n\n\n\nDec 29, 2024\n\n\nMahamadi NIKIEMA\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nMahamadi NIKIEMA\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "things_i_learned/create_class_with_type/index.html",
    "href": "things_i_learned/create_class_with_type/index.html",
    "title": "Classes Are Instances of type",
    "section": "",
    "text": "When using the function type on an object, Python will tell us which instance this object belongs to.\n\nx = [1, 2, 3]\nprint(type(x))\n\n&lt;class 'list'&gt;\n\n\nIf we apply the type method to a class like list, we get:\n\nprint(type(list))\nprint(type(type(x)))\n\n&lt;class 'type'&gt;\n&lt;class 'type'&gt;\n\n\nA class is an instance of type and we can create a class from type.\ntype can take three arguments:\ntype(classname, superclasses, attributes_dict)\n\nclassname is the name of the class\nsuperclasses is a tuple of the superclasses\nattributes_dict is the dictionary attribute of the class.\n\n\nclass Dog:\n    pass\n\ndog = Dog()\nprint(type(dog))\n\n&lt;class '__main__.Dog'&gt;\n\n\nThe same creation using type, passing an attribute a, and adding a method bark to the class:\n\nDog = type(\"Dog\", (), {\"a\":1, \"bark\": lambda self: \"woof\"})\nd = Dog()\nprint(type(d))\nprint(d.a)\nprint(d.bark())\n\n&lt;class '__main__.Dog'&gt;\n1\nwoof\n\n\nWe can access the attribute a and call the method bark, which was not possible during the first creation of the class.\nEven if it is uncommon, we can use type to dynamically create classes. The class keyword is basically another way to create a class; under the hood, Python uses type."
  },
  {
    "objectID": "things_i_learned/test/index.html",
    "href": "things_i_learned/test/index.html",
    "title": "Serialisation with Msgspec",
    "section": "",
    "text": "While Pydantic is well-known for serialization and validation in Python, I recently discovered msgspec, a lightning-fast library that supports encoding and decoding various formats, including JSON, YAML, TOML, and MessagePack."
  },
  {
    "objectID": "things_i_learned/test/index.html#encoding",
    "href": "things_i_learned/test/index.html#encoding",
    "title": "Serialisation with Msgspec",
    "section": "Encoding",
    "text": "Encoding\nYou can encode Python objects into JSON or MessagePack.\n\nimport msgspec\n\n# Encoding as JSON\njson_data = msgspec.json.encode({\"name\": \"awesome name\"})\nprint(json_data)\n\n# Encode as msgpack\nmsgpack_data = msgspec.msgpack.encode({\"name\": \"awesome name\"})\nprint(msgpack_data)\n\nb'{\"name\":\"awesome name\"}'\nb'\\x81\\xa4name\\xacawesome name'"
  },
  {
    "objectID": "things_i_learned/test/index.html#the-core-msgspec.struct",
    "href": "things_i_learned/test/index.html#the-core-msgspec.struct",
    "title": "Serialisation with Msgspec",
    "section": "The Core: msgspec.Struct",
    "text": "The Core: msgspec.Struct\nThe core component is the module msgspec.Struct.\nAt the heart of msgspec is the Struct class, which provides structure and type safety for your data models.\n\nDefining a Structured Mode\n\nimport msgspec\nfrom typing import Set\n\n\nclass ConfigStrategy(msgspec.Struct):\n    name: str\n    language: str\n    stop_words: Set[str] = set()\n\n\nspacy_cfg = ConfigStrategy(name=\"spacy\", language=\"french\")\nprint(spacy_cfg)\n\nConfigStrategy(name='spacy', language='french', stop_words=set())\n\n\nEncoding the data\nYou can encode the structured object directly into JSON:\n\nmsgspec.json.encode(spacy_cfg)\n\nb'{\"name\":\"spacy\",\"language\":\"french\",\"stop_words\":[]}'\n\n\nDecoding the data\nJSON Decoding\nBy default, msgspec does not perform type validation during the decoding:\n\nmsgspec.json.decode(b'{\"name\":\"spacy\",\"language\":\"french\",\"stop_words\":[]}')\n\n{'name': 'spacy', 'language': 'french', 'stop_words': []}\n\n\nType Validation\nmsgspec makes it easy to decode serialized data into a structured object, complete with type validation:\n\nmsgspec.json.decode(b'{\"name\":\"spacy\",\"language\":\"french\",\"stop_words\":[]}', type=ConfigStrategy)\n\nConfigStrategy(name='spacy', language='french', stop_words=set())\n\n\nIf you’re looking for a high-performance alternative to libraries like Pydantic, give msgspec a try!"
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html",
    "href": "posts/layernorm-rmsnorm/normalization.html",
    "title": "Normalization in Transformers",
    "section": "",
    "text": "Normalization techniques are widely used in Deep Learning. In Transformers, normalization is applied at various points to maintain stable gradients and enable faster convergence. They are used in the following ways:\nKey benefits of normalization:\n\nPrevents overfitting.\nImproves generalization.\nStabilizes training dynamics.\nBoosts performance on large-scale tasks."
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#introduction",
    "href": "posts/layernorm-rmsnorm/normalization.html#introduction",
    "title": "Normalization in Transformers",
    "section": "",
    "text": "Normalization techniques are widely used in Deep Learning. In Transformers, normalization is applied at various points to maintain stable gradients and enable faster convergence. They are used in the following ways:\nKey benefits of normalization:\n\nPrevents overfitting.\nImproves generalization.\nStabilizes training dynamics.\nBoosts performance on large-scale tasks."
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#normalization-methods-in-transformers",
    "href": "posts/layernorm-rmsnorm/normalization.html#normalization-methods-in-transformers",
    "title": "Normalization in Transformers",
    "section": "Normalization Methods in Transformers",
    "text": "Normalization Methods in Transformers\nThere are many normalization methods in Transformers. Some of them are:\n\nLayer Normalization (LayerNorm)\nRoot Mean Square Normalization (RMSNorm)"
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#layernorm",
    "href": "posts/layernorm-rmsnorm/normalization.html#layernorm",
    "title": "Normalization in Transformers",
    "section": "LayerNorm",
    "text": "LayerNorm\nLayerNorm is a normalization method that is used in the original Transformer model. It is used to normalize the input to the model. The formula for LayerNorm is:\n\\[\ny = \\gamma \\times \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta\n\\]\n\n\\(x\\): Input tensor\n\n\\(\\mu\\): Mean of the features\n\n\\(\\sigma\\): Standard deviation of the features\n\n\\(\\epsilon\\): Small constant for numerical stability\n\n\\(\\gamma\\), \\(\\beta\\): Learnable scale and bias parameters\n\nPyTorch Implementation:\nimport torch\nimport torch.nn as nn\n\nclass SimpleLayerNorm(nn.Module):\n    def __init__(self, layer_shape, eps: float = 1e-6) -&gt; None:\n        super().__init__()\n        self.layer_shape = (layer_shape,) if isinstance(layer_shape, int) else tuple(layer_shape)\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(*self.layer_shape))\n        self.beta = nn.Parameter(torch.zeros(*self.layer_shape))\n\n    def forward(self, x: torch.Tensor):\n        start_dim = x.dim() - len(self.layer_shape)\n        norm_dim = tuple(range(start_dim, x.dim()))\n        variance = x.var(norm_dim, keepdim=True, unbiased=False)\n        mean = x.mean(norm_dim, keepdim=True)\n        norm_x = (x - mean) / torch.sqrt(variance + self.eps)\n        norm_x = self.gamma*norm_x + self.beta\n        return norm_x"
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#rmsnorm",
    "href": "posts/layernorm-rmsnorm/normalization.html#rmsnorm",
    "title": "Normalization in Transformers",
    "section": "RMSNorm",
    "text": "RMSNorm\nRMSNorm is a more recent variant used in models like LLaMA-3 and Qwen-3. It removes the mean subtraction step, normalizing only by the Root Mean Square (RMS) of activations.\n\\[\ny = \\frac{x}{\\sqrt{\\epsilon + \\sum_{i=1}^n x_i^2}}\\] where \\(x\\) is the input, \\(y\\) is the output, and \\(\\epsilon\\) is a small constant.\nIn Python, you can use the following code to implement RMSNorm:\nimport torch\nimport torch.nn as nn\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-6, bias=False, qwen3_compatible=True):\n        super().__init__()\n        self.eps = eps\n        self.qwen3_compatible = qwen3_compatible\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n\n    def forward(self, x):\n        input_dtype = x.dtype\n\n        if self.qwen3_compatible:\n            x = x.to(torch.float32)\n\n        variance = x.pow(2).mean(dim=-1, keepdim=True)\n        norm_x = x * torch.rsqrt(variance + self.eps)\n        norm_x = norm_x * self.scale\n\n        if self.shift is not None:\n            norm_x = norm_x + self.shift\n\n        return norm_x.to(input_dtype)\nRMSNorm is computationally more efficient than LayerNorm and is used in many models today such as QWen-3 and LLaMA-3."
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#conclusion",
    "href": "posts/layernorm-rmsnorm/normalization.html#conclusion",
    "title": "Normalization in Transformers",
    "section": "Conclusion",
    "text": "Conclusion\nNormalization is critical for stable and efficient Transformer training. While LayerNorm remains the standard choice, RMSNorm is increasingly popular in large-scale LLMs for its computational efficiency."
  },
  {
    "objectID": "posts/layernorm-rmsnorm/normalization.html#further-reading",
    "href": "posts/layernorm-rmsnorm/normalization.html#further-reading",
    "title": "Normalization in Transformers",
    "section": "Further Reading",
    "text": "Further Reading\n\nRMSNorm: The Root Mean Square Layer Normalization\nLayer Normalization\nAttention is all you need"
  },
  {
    "objectID": "draft/data_collators.html",
    "href": "draft/data_collators.html",
    "title": "Outline",
    "section": "",
    "text": "from torch.utils.data import DataLoader\nfrom transformers import DefaultDataCollator, AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerBase\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ntokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntext: list[str] = [\"A cute dog\", \"My cat is cute\"]\nWe can tokenize the text using the tokenizer and pass it to the data collator.\ntokens: list[dict[str, list[int]]] = [tokenizer(t) for t in text]\nprint(tokens)\n\n[{'input_ids': [49, 25489, 2767], 'attention_mask': [1, 1, 1]}, {'input_ids': [5965, 2644, 314, 25489], 'attention_mask': [1, 1, 1, 1]}]\nThe first sentence has 3 tokens, the second has 4 tokens. This will not work for tensor batches, which require all samples to have the same length.\nThe default data collator in Hugging Face is initialized as follows"
  },
  {
    "objectID": "draft/data_collators.html#defaultdatacollator",
    "href": "draft/data_collators.html#defaultdatacollator",
    "title": "Outline",
    "section": "DefaultDataCollator",
    "text": "DefaultDataCollator\n\ncollate_fn = DefaultDataCollator()\n\n\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    print(batch)\n    break\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[7], line 2\n      1 dataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\n----&gt; 2 for batch in dataloader:\n      3     print(batch)\n      4     break\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733, in _BaseDataLoaderIter.__next__(self)\n    730 if self._sampler_iter is None:\n    731     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    732     self._reset()  # type: ignore[call-arg]\n--&gt; 733 data = self._next_data()\n    734 self._num_yielded += 1\n    735 if (\n    736     self._dataset_kind == _DatasetKind.Iterable\n    737     and self._IterableDataset_len_called is not None\n    738     and self._num_yielded &gt; self._IterableDataset_len_called\n    739 ):\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789, in _SingleProcessDataLoaderIter._next_data(self)\n    787 def _next_data(self):\n    788     index = self._next_index()  # may raise StopIteration\n--&gt; 789     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    790     if self._pin_memory:\n    791         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n---&gt; 55 return self.collate_fn(data)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:125, in DefaultDataCollator.__call__(self, features, return_tensors)\n    123 if return_tensors is None:\n    124     return_tensors = self.return_tensors\n--&gt; 125 return default_data_collator(features, return_tensors)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:93, in default_data_collator(features, return_tensors)\n     87 # In this function we'll make the assumption that all `features` in the batch\n     88 # have the same attributes.\n     89 # So we will look at the first element as a proxy for what attributes exist\n     90 # on the whole batch.\n     92 if return_tensors == \"pt\":\n---&gt; 93     return torch_default_data_collator(features)\n     94 elif return_tensors == \"tf\":\n     95     return tf_default_data_collator(features)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:159, in torch_default_data_collator(features)\n    157             batch[k] = torch.from_numpy(np.stack([f[k] for f in features]))\n    158         else:\n--&gt; 159             batch[k] = torch.tensor([f[k] for f in features])\n    161 return batch\n\nValueError: expected sequence of length 3 at dim 1 (got 4)\n\n\n\nWe are getting errors because the data samples have different lengths. The default data collator in Hugging Face is very simple. - It does not handle variable length data. - It does not handle padding.\nTo solve this problem, we can use a padding technique to get the same length for all samples in a batch. Hugging Face provides a built-in data collator called DataCollatorWithPadding that can handle this padding automatically. The DataCollatorWithPadding handle tokenization and padding automatically."
  },
  {
    "objectID": "draft/data_collators.html#datacollatorwithpadding",
    "href": "draft/data_collators.html#datacollatorwithpadding",
    "title": "Outline",
    "section": "DataCollatorWithPadding",
    "text": "DataCollatorWithPadding\n\nfrom transformers import DataCollatorWithPadding\ncollate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    print(input_ids)\n    print(batch.keys())\n    decoded = tokenizer.batch_decode(input_ids)\n    print(decoded)\n    break\n\ntensor([[   49, 25489,  2767,     2],\n        [ 5965,  2644,   314, 25489]])\ndict_keys(['input_ids', 'attention_mask'])\n['A cute dog&lt;|im_end|&gt;', 'My cat is cute']\n\n\n\nWe can see that we did not get any errors and the first example has been padded with 2 at the last position to match the length of the second example.\n\nFor Nvidia GPUs, it is often more efficient to pad the sequences to a multiple of 8. This can be done by setting the pad_to_multiple_of parameter when initializing the DataCollatorWithPadding.\n\nfrom transformers import DataCollatorWithPadding\ncollate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    print(input_ids)\n    print(batch.keys())\n    decoded = tokenizer.batch_decode(input_ids)\n    print(decoded)\n    break\n\ntensor([[   49, 25489,  2767,     2,     2,     2,     2,     2],\n        [ 5965,  2644,   314, 25489,     2,     2,     2,     2]])\ndict_keys(['input_ids', 'attention_mask'])\n['A cute dog&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;', 'My cat is cute&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;&lt;|im_end|&gt;']\n\n\nUsing pad_to_multiple_of=8 we have all tensors with 8 values. If you are fine-tuning a decoder model for language modeling, the collator used is DataCollatorForLanguageModeling. Let’s see how it works by importing the DataCollatorForLanguageModeling"
  },
  {
    "objectID": "draft/data_collators.html#datacollatorforlanguagemodeling",
    "href": "draft/data_collators.html#datacollatorforlanguagemodeling",
    "title": "Outline",
    "section": "DataCollatorForLanguageModeling",
    "text": "DataCollatorForLanguageModeling\n\nfrom transformers import DataCollatorForLanguageModeling\n\nWe can use it by setting mlm to False. mlm=False means that we are fine-tuning a decoder model for language modeling and not a masked language model such as BERT.\n\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ndataloader = DataLoader(tokens, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    all_tokens = tokenizer.batch_decode(input_ids)\n    print(batch)\n    for token in all_tokens:\n        print(token)\n    break\n\n{'input_ids': tensor([[   49, 25489,  2767,     2],\n        [ 5965,  2644,   314, 25489]]), 'attention_mask': tensor([[1, 1, 1, 0],\n        [1, 1, 1, 1]]), 'labels': tensor([[   49, 25489,  2767,  -100],\n        [ 5965,  2644,   314, 25489]])}\nA cute dog&lt;|im_end|&gt;\nMy cat is cute\n\n\nBy default, the data collator pads the sequences to the longest sequence in the batch and the side is right. We can use pad_to_multiple_of=8 as in the previous collator.\nWe got an additional key in the dictionnary because it used to make next-token-prediction during the training or finetuning.\nWe would like to add an extra varaible reference for the reference text without tokenizing it for evaluation purpose.\nFor this,let us add the french translation of the two english sentences.\nTrainer evaluation loop computes the loss but we want to evaluate the model using some metrics like BLEU. For this we need to pass the source in the model forward in the form of input_ids and the reference in the form of labels. And decode the model output using the tokenizer to get the text. But the data collator is not allowing us to pass other keys like the reference. We have two options here: 1. Design a PyTorch Dataset that returns only the input_ids and labels. 2. Design a custom data collator that handles extra variables like the reference. If the dataset is in HuggingFace Dataset format, it is simple to subclass the data collator and add a new method __call__. If the dataset is not in HuggingFace Dataset format, it is more complex to subclass the data collator without writing a PyTorch Dataset.\n\n\ntext_fr: list[str] = [\"Un chien mignon\", \"Mon chat est mignon\"]\nparallel_text: list[dict[str, str | int]] = []\nfor src, ref in zip(text, text_fr):\n    tokenized = tokenizer(src)\n    tokenized[\"reference\"] = ref\n    parallel_text.append(tokenized)\n\nprint(parallel_text)\n\n[{'input_ids': [49, 25489, 2767], 'attention_mask': [1, 1, 1], 'reference': 'Un chien mignon'}, {'input_ids': [5965, 2644, 314, 25489], 'attention_mask': [1, 1, 1, 1], 'reference': 'Mon chat est mignon'}]\n\n\nNow, let’s see what happens if we add this column.\n\n\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ndataloader = DataLoader(parallel_text, batch_size=2, collate_fn=collate_fn)\nfor batch in dataloader:\n    print(batch)\n    break\n    \n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:777, in BatchEncoding.convert_to_tensors(self, tensor_type, prepend_batch_axis)\n    776 if not is_tensor(value):\n--&gt; 777     tensor = as_tensor(value)\n    779     # Removing this for now in favor of controlling the shape with `prepend_batch_axis`\n    780     # # at-least2d\n    781     # if tensor.ndim &gt; 2:\n    782     #     tensor = tensor.squeeze(0)\n    783     # elif tensor.ndim &lt; 2:\n    784     #     tensor = tensor[None, :]\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:739, in BatchEncoding.convert_to_tensors.&lt;locals&gt;.as_tensor(value, dtype)\n    738     return torch.from_numpy(np.array(value))\n--&gt; 739 return torch.tensor(value)\n\nValueError: too many dimensions 'str'\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[12], line 3\n      1 collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n      2 dataloader = DataLoader(parallel_text, batch_size=2, collate_fn=collate_fn)\n----&gt; 3 for batch in dataloader:\n      4     print(batch)\n      5     break\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733, in _BaseDataLoaderIter.__next__(self)\n    730 if self._sampler_iter is None:\n    731     # TODO(https://github.com/pytorch/pytorch/issues/76750)\n    732     self._reset()  # type: ignore[call-arg]\n--&gt; 733 data = self._next_data()\n    734 self._num_yielded += 1\n    735 if (\n    736     self._dataset_kind == _DatasetKind.Iterable\n    737     and self._IterableDataset_len_called is not None\n    738     and self._num_yielded &gt; self._IterableDataset_len_called\n    739 ):\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789, in _SingleProcessDataLoaderIter._next_data(self)\n    787 def _next_data(self):\n    788     index = self._next_index()  # may raise StopIteration\n--&gt; 789     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    790     if self._pin_memory:\n    791         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\n     53 else:\n     54     data = self.dataset[possibly_batched_index]\n---&gt; 55 return self.collate_fn(data)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:46, in DataCollatorMixin.__call__(self, features, return_tensors)\n     44     return self.tf_call(features)\n     45 elif return_tensors == \"pt\":\n---&gt; 46     return self.torch_call(features)\n     47 elif return_tensors == \"np\":\n     48     return self.numpy_call(features)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:1013, in DataCollatorForLanguageModeling.torch_call(self, examples)\n   1010     self.create_rng()\n   1012 if isinstance(examples[0], Mapping):\n-&gt; 1013     batch = pad_without_fast_tokenizer_warning(\n   1014         self.tokenizer, examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of\n   1015     )\n   1016 else:\n   1017     batch = {\n   1018         \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n   1019     }\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:67, in pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs)\n     64 tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n     66 try:\n---&gt; 67     padded = tokenizer.pad(*pad_args, **pad_kwargs)\n     68 finally:\n     69     # Restore the state of the warning.\n     70     tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3407, in PreTrainedTokenizerBase.pad(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\n   3404             batch_outputs[key] = []\n   3405         batch_outputs[key].append(value)\n-&gt; 3407 return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:241, in BatchEncoding.__init__(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\n    237     n_sequences = encoding[0].n_sequences\n    239 self._n_sequences = n_sequences\n--&gt; 241 self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n\nFile ~/OpenSource/mnikiema.github.io/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:793, in BatchEncoding.convert_to_tensors(self, tensor_type, prepend_batch_axis)\n    788         if key == \"overflowing_tokens\":\n    789             raise ValueError(\n    790                 \"Unable to create tensor returning overflowing tokens of different lengths. \"\n    791                 \"Please see if a fast version of this tokenizer is available to have this feature available.\"\n    792             ) from e\n--&gt; 793         raise ValueError(\n    794             \"Unable to create tensor, you should probably activate truncation and/or padding with\"\n    795             \" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\n    796             f\" features (`{key}` in this case) have excessive nesting (inputs type `list` where type `int` is\"\n    797             \" expected).\"\n    798         ) from e\n    800 return self\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`reference` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n\n\n\nThe data collator is trying to convert all the variables in the dataset to tensors. Since the reference variable is a string, it cannot be converted to a tensor and we get an error. To solve this problem, we can subclass the DataCollatorForLanguageModeling and override the __call__ method to handle the extra variables.\nNow we know how to use the data collator for instruction fine-tuning. My use case is to fine-tune a decoder model for translation and we want to evaluate it using some metrics like BLEU. For this, we firstly rely on pandas loop to handle the batching and we want to use the data collator to tokenize the text. How to pass the text and reference to the data collator without loosing some variable like the reference? For this I subclass the data collator and add a new method __call__."
  },
  {
    "objectID": "draft/data_collators.html#customize-datacollator",
    "href": "draft/data_collators.html#customize-datacollator",
    "title": "Outline",
    "section": "Customize DataCollator",
    "text": "Customize DataCollator\n\nclass TranslationPromptCollator(DataCollatorWithPadding):\n    def __init__(\n        self,\n        tokenizer: PreTrainedTokenizerBase,\n        pad_to_multiple_of: int | None = None,\n        return_tensors: str = \"pt\",\n        max_length: int | None = None,\n    ):\n        super().__init__(\n            tokenizer=tokenizer,\n            padding=True,\n            pad_to_multiple_of=pad_to_multiple_of,\n            return_tensors=return_tensors,\n            max_length=max_length,\n        )\n        self.tokenizer = tokenizer\n\n    def __call__(self, features):\n        src_texts = [f[\"src_text\"] for f in features]\n        tgt_texts = [f[\"tgt_text\"] for f in features]\n\n        examples = {\n            \"source\": src_texts,\n            \"target\": tgt_texts,\n        }\n\n        inputs = self.tokenizer(\n            examples[\"source\"],\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n            padding_side=\"left\",\n        )\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"],\n            \"references\": tgt_texts,\n            \"sources\": src_texts,\n        }\n\nLet us create some sample data with source and reference text. We can use a simple list of dictionaries for this because it behaves like a HuggingFace Dataset.\n\n\ndata = [\n    {\n        \"src_text\": text[0],\n        \"tgt_text\": text_fr[0],\n        },\n        {\n        \"src_text\": text[1],\n        \"tgt_text\": text_fr[1],\n        },\n    ]\n\n\ntranslation_collator = TranslationPromptCollator(tokenizer=tokenizer)\ndataloader = DataLoader(data, batch_size=2, collate_fn=translation_collator)\nfor batch in dataloader:\n    print(batch)\n    break\n\n{'input_ids': tensor([[    2,    49, 25489,  2767],\n        [ 5965,  2644,   314, 25489]]), 'attention_mask': tensor([[0, 1, 1, 1],\n        [1, 1, 1, 1]]), 'references': ['Un chien mignon', 'Mon chat est mignon'], 'sources': ['A cute dog', 'My cat is cute']}\n\n\nThe final loop for evaluation looks like this:\n\n\nfor batch in dataloader:\n    references = batch.pop(\"references\")\n    sources = batch.pop(\"sources\")\n    prediction = model.generate(**batch, max_length=50)\n    decoded_preds = tokenizer.batch_decode(prediction, skip_special_tokens=True)\n    for ref, pred, src in zip(references, decoded_preds, sources):\n        print(f\"Reference: {ref} --&gt; Prediction: {pred} --&gt; Source: {src}\")\n\nReference: Un chien mignon --&gt; Prediction: A cute dog, and I'm excited to see what the day brings.\n\nI've been working on the dog's training, making sure he's comfortable and happy. I've also been thinking about how to make the day even more special --&gt; Source: A cute dog\nReference: Mon chat est mignon --&gt; Prediction: My cat is cute and playful, and I love watching her play with me. She's a bit of a wild animal, but she's got a heart of gold and loves to chase after things. She's a bit of a rescue, too - --&gt; Source: My cat is cute"
  },
  {
    "objectID": "draft/data_collators.html#conclusion",
    "href": "draft/data_collators.html#conclusion",
    "title": "Outline",
    "section": "Conclusion",
    "text": "Conclusion"
  }
]